1-й слайд

Всем привет! Меня зовут Андрей и я Computer Vision - голик и состою в группе punch-to-face. 

Сегодня мы поговорим о стремительно набирающей обороты сфере 3Д реконструкции, а именно о применениях ее методов к боям ММА - смешных боевых искувств.

2-й слайд

У тех кто не был на просторах слака ods.ai, не видел наш фирменный эмоджи или не клюнул на навязчивую рекламу Андрея  Спиридонова в каналах #careers, #random_talks, #ods_pet_projects, наверно появился вопрос - а что такое хаштаг punch-to-face. 

Ну во первых это канал в слаке ods.ai посвященный 3Д реконструкции на который подписано 269 человек, В этом канале можно найти много интересных обсуждений замечательных людей завязанных не только на ММА.

Во вторых это канал в телеграмме, где тусуется 13 человек и общаются уже исключительно о ММА… ну и немного о других вещах о которых стыдно говорить на публике. 

Ну и самое главное, punch-to-face это амбициозный проект, который хочет привнести в бои без правил немного того, что мы все здесь так любим - машинное обучение и Data Science. 

Мы работаем над созданием интернет портала, в котором можно было бы посмотреть бои всех организаций с такими крутыми фишками как возможность смотреть бои с разных ракурсов, возможность смотреть бои от первого лица, умной ненавязчивой рекламой и интерактивной статистикой

3-й слайд

Наш проект еще достаточно молод  и мы сейчас на стадии формирования и предварительного тестирования гипотез. 

Из наших достижений на данный момент хочу выделить то, что мы собрали датасет из 300 примеров с очень хорошей разметкой для тестирования гипотез семантической сегментации. Для справки, у нас есть 4 класса: пол, люди, информационное табло и фон.  Также на наших данных можно работать с паноптической сегментацией людей, так как они закрашены в разные цвета

4-й слайд

Еще мы отсняли постановочный бой с 23 камер для тестирования гипотез multi-view реконструкции и накачали видео в высоком разрешении с ютуба для работы с одной камеры. Вот можете посмотреть на слайде yf yfie cmtvre

Мы общаемся и договариваемся о сотрудничестве с ММА организациями и даже наш десант присутствовал на турнире ММА неделю назад в Краснодаре, где мы добыли много интересного материала в высоком качества.

Все данные которые я упомянул находятся в открытом доступе, но пока без лицензии. Мы работаем над тем чтобы опубликовать их в формате kaggle datasets, а пока вы можете найти ссылки в наше канале ods.ai

5-й слайд

Довольно наглой рекламы, перейдем к делу - что нужно сделать чтобы восстановить сцену боя ММА? 

Во первых нужно восстановить 3-мерную модель человека

3Д модель или по-другому мэш - это набор точек в трехмерном пространстве объединеные отрезками - по-другому ребрами. Чем больше многоугольников в модели - тем красивее и плавнее она получается.

Во вторых, после того как мы восстановили модель - нам нужно ее разукрасить. Для этого нам нужно две вещи - это текстура и UV-карта. 

Текстура - это обычный двух мерный рисунок, который похож на вот эту картинку. 

UV-карта - это карта соответствий точек на текстуре и точек на модели. С помощью ее мы наклеиваем текстуру на нашу модель.

Напоследок, готовую модель нужно поместить в сцену. По 2-мерной картинке узнать где находится 3-мерный координаты модели это не всегда такая простая задача как кажется.

Повторите эти шаги 3 раза на каждый кадр ( по разу на рефери и двух бойцов) - и вуаля  - мы получили 3Д реконструкцию боя.

6-й слайд

На этом слайде я собрал список лучших методов в открытом доступе для решения задач связанных с 3Д реконструкцией людей, и также оставил небольшой комментарий по каждому из них. 

Большинство работает на сверхточных нейронных сетях, о многих я сегодня расскажу, о других вы можете узнать пройдя по ссылкам на последнем слайде, как только презентация станет доступна для скачивания. 

Для многих систем из этого списка нужно потратить много времени чтобы их запустить, потому что у них много dependencies. 

К счастью, в большинстве случаев кто-то из замечательных подписчиков нашего канала уже пытался запустить нужную вам систему, поэтому не стесняйтесь спрашивать о помощи в #proj_punch_to_face

7-й слайд

Вообще, я хотел сделать этот доклад что-то вроде полуворкшопа, где я детально расскажу через что нужно пройти чтобы грубо восстановить движения одного бойца по видео с одной камеры.

Поэтому следующие 20 минут пройдут таким обрзом:

Сначала я расскажу о особенностях реконструкции боев ММА, чем она отличается от реконструкции например футбола или создания виртуальной примерочной.

Потом я детально пройдусь по каждой из трех подзадач реконструкции, расскажу какие существуют state-of-the-art подходы, чего им не хватает и что делается чтобы восполнить эти пробелы. 

В секции реконструкции меша я еще выделю немного времени для обсуждения трекинга ключевых точек скелета, потому что я считаю эту задачу исключительно важной для общего успеха пайплайна.

Если время будет позволять, я расскажу о возможных улучшениях получившейся грубой реконструкции, которые требует решения задачи паноптической сегментации бойцов.

Вторым бонусом я расскажу о другом подходе к реконструкции, где не нужно восстанавливать модель на каждый кадр, а можно подготовить модель перед боем и потом просто двигать ее с помощью детектора 3Д скелета.

Но скорее всего я не успею перейти к бонусам и мне придется оформить материал как два поста в наш канал ОДС, что будет вам дополнительны стимулом  подписаться на нас в слаке.

8-й слайд

В чем же особенности реконструкции ММА?

Во первых мы реконструируем людей. В этом есть свои плюсы и минусы. Минус в том что тело человека это очень сложный объект, по сравнению например с кубом. 

Плюс в том, что люди уже давно интересуются реконструкцией себя, поэтому мы можем воспользоваться множеством готовых моделей и подходов.

Вторая особенность - это фиксированное количество людей в сцене. Реконструкция двух-трех людей (двух бойцов и рефери) это конечно сложнее чем реконструкция одного человека, но все же не толпа. 

Мы можем оптимизировать некоторые алгоритмы используя информацию, что у нас два человека в кадре. Если останется время я покажу это на примере паноптической сегментации.

Третья особенность это динамичность сцены. Текстуры бойцов и пола постояло меняются, например из-за освещения, ну и иногда из-за крови. 

Ну и самая ужасная особенность ММА это occlusion или по-русски перекрытие, что видно на примере трех картинок на этом слайде. J,mZXC

 Хочу обратить ваше внимание на кадр в правом нижнем углу. Чтобы реконструировать этот кадр, нейронной сетке нужно определить какая нога относится к какому бойцу. 

Научить сетку делать это - очень сложная задача, особенно если тренировать на HD картинках на кропах. Ведь чтобы понять где чья нога, даже человека нужно иметь понимание всей сцены. 

Да в принципе, я сейчас и сам неуверен где там чья нога… hlihlk

9-й слайд

Теперь имея ввиду эти факторы, мы можем обсудить техники воссоздания модели. 

Многие системы реконструкции меша используют трекинг ключевых точек позы  как часть пайплайна. Позу используют как фичи, иногда для псевдолейблинга или для вырезания людей из кадра. 

Использование ошибки 2Д репроэкции скелета как часть лосса - это стандарт современной реконструкции.

Дополнительно, но не обязательно, в реконструкции можно использовать информацию о силуэте которую можно получить  с помощью паноптической сегментации. 

Силуэт можно использовать как часть лосса, но еще можно закрасить фон, облегчая задачу нейронной сети. Особенно это докидывает если на фоне есть другой человек, потому что это реально путает нейронные сети и может получиться что-то похожее на эту картинку.

Получив информацию о скелете или силуэте можно переходить к основной задаче - регессии вершин модели.

Эффективно регрессия подбирает координаты вершин так, чтобы  получившиеся модель максимально соответствовала с той визуальной информацией что мы получаем с кадра.

В результате мы получим что-то вроде на вот этой картинке, возможно еще более детализированное и красивое

10-й слайд

Трекинг 2Д позы это в принципе самая простая и наиболее решенная задача из всех что я буду обсуждать. 

В этом вы можете убедиться сами из гифки внизу. Да может результаты моментами и выглядят немного шумными, но их вполне можно сгладить дополнительными евристиками.

Есть два вида детекторов 2Д позы - bottom-up и top-down. 

Top-down сначала детектит прямоугольник с человеком, потом находит ключевые точки.
 
Пример такого детектора MaskRCNN в keypoint режиме, когда после детекции человека, делается операция RoI Allign (вырезается bbox и меняется размер вырезанной картинки) и потом fully-convolutional нейронная сеть находит координаты ключевых точек.

Отличие Bottom-up методов в том, что они сначала находят точки, а потом группируют их в людей.

Трекинг (когда мы работает уже не с одним кадром, а с целым видео) - намного легче делать top-down методами, потому что между соседними кадрами нужно только находить соответствие на уровне bbox-ов, а не по каждой точке, что намного уменьшает complexity задачи.

В целом и в общем Top-Down методы с большим отрывом побеждают как на датасетах с отдельными кадрами (COCO) так и с трекингом (PoseTrack), c двумя большими но:

они плохо улавливают сложные позы например прыжок или удар ногой в ММА
они очень плохо справляются с перекрытием, чаще всего просто игнорируя перекрытого человека

К сожалению этого добра в ММА хватает, и из-за этого трекинг с помощью top-down методов довольно быстро ломается, как видно на вот этой гифке. 

Алгоритм просто теряется и не может сново вернутся туда где находится боец.

Поэтому в нашей проблеме стоит присмотреться как и к top-down методам так и к bottom-up. 

Я думаю что это было бы хорошей PhD работой - найти золотую середину между этими двумя подходами, например научиться “переключатсья” между ними, с помощью возможно умного алгоритма трекинга.

C топовым системами детектирования позы вы можете ознакомится на слайде с ссылками.

11-й  слайд

Перед тем как я перейду непосредственно к обсуждению реконструкции меша, мне нужно представить вам самую популярную статистическую модель человеческого тела - SMPL. 

Эта модель берет на вход 85 параметров - включая 72 параметра позы, 10 формы и 3 параметра ракурса. На выходе у нее довольно детализированный меш с около семи тысячью вершинами и около 14 тысячью  ребрами.

Сама по себе модель выглядит в целом неочень, например она не может передать волосы или одежду. 

Но многие методы используют вариант этой модели SMPL-D или SMPL-X и дооптимизруют положение вершин чтобы передать детали, которая SMPL сама по себе передать к сожаление не может.


12-й слайд 

Два из представленных топовых методов на этой слайде - именно так и делают, это  GraphCMR и Tex2Shape. Третий метод просто хорошо умеет находить эти 85 параметров. То есть то мы имеем,  разные формы SMPL модели это де-факто SОТА в реконструкции меша.

В целом основная проблема реконструкции меша - это катастрофически дефицит исходных данных для тренировки. 

Это обосновывается тем, что это очень дорого заставить человека красиво разметить меш в тонких деталях. А в in-the-wild применениях, как например ММА, вообще не предоставляется возможным.

В таких случаях есть два выхода. Первый использовать UP-3Д данные, которые используют другой базовый метод SMPLify, подгоняют SMPL модель и отбирают самые лучшие результаты. На таких данных натренирован GraphCMR. 

Другой выход - покупать четкие 3Д сканы людей и аугментировать их - повращать например. Так натренирован Tex2Shape. 

Даже не разбираясь в деталях этих двух подходов, можно заранее сказать что более красивые результаты будет показывать (как вы думаете какой метод этот или этот?) - Tex2Shape, потому что 3Д сканы это более точные данные, а GraphCMR просто тренируется воспроизводить SMPL.

Теперь к деталям:
Human Motion and Mesh Recovery - это SOTA по реконструкции видео, пока единственная работа которая выдает плавные движения меша. В других подходах анимация получается немного дерганая. 

Авторы достигли такого эффекта использовав temporal encoder - это 1-мерная сверточная сеть на вход который идет вектор из 2048 фич ResNet-а. Это сеть эффективно использует информацию о соседних кадрах для сглаживания.

Второй трюк - это использование временного компонента (дельты SMPL параметров) в лоссе.

Третий трюк - это использование псеволейблинга. 

Авторы собрали свой неразмеченный датасет из видео InstaVariety, и прогнали на нем OpenPose - детектор 2Д позы о которых я рассказывал несколько слайдов ранее.

Потом эти псевдо-ground truth позы использовались для тренировки и заметно улучшали результат.

Минус - это то что на выходе SMPL.

Подход GraphCMR - использует иерархическую структуру модели SMPL (точнее знание какие вершины с какими связаны) и Graph Convolutions для предсказания координат вершин модели.

 Я попытаюсь очень просто обьяснить как работают Graph Convolutions возможно немного пожертвую правдоподобностью.  В общем, эти свертки смотрят не только на саму вершину, но и еще на соседние вершины, и усредняют результаты.

Плюсы этого подхода, то что он очень гибкий - может работать с разными фичами на входе. Еще мэш на выходе не ограничен возможностями SMPL модели.
 
Минус - что претренированая модель натренирована подражать SMPL, и поэтому неспособна на что-то намного красивее дефолтной модели

Наконец создатели Tex2Shape поступили еще хитрее - они использовали неполную текстуру из детектора текстуры как входные данные для сетки pix2pix, которая используется для перевода картинок.

На выходе у сетки - карта разницы между координатами SMPL  и настоящей модели. 

Результат очень поражает - вот можете посмотреть на картинку, очень детализированная реконструкция включая одежду и волосы.

Все существующие методы не работают при перекрытии, выдавая иногда что-то веселое как например на этой гифке.

Для того чтобы сделать что-то хорошее в таких условиях, нужно решить проблему паноптической сегментации, что я тоже думаю будет интересной темой для PhD  или магистерской работы

13-й слайд

Переходим к текстурам. Для реконструкции текстуры нам нужна более детальная сегментация чем для реконструкции меша.

 Как показывает практика, нейронной сети намного легче учить текстуры какой-то части тела, чем сразу всего человека.  
 Поэтому и нужна сегментация частей тела: передней части руки, задней части руки, ладоней и так далее.

Потом мы должны выучить функцию которая будет на каждый пиксель отсегментированной части тела находить соответствующий этому пикселю пиксель в текстуре.

Но с одного кадра мы получим только неполную текстуру.

В каждом кадре мы видим только одну часть тела, например на этой картинке мы не видим спину или затылок. 

Из-за этого нужно думать как объединять текстуры с нескольких кадров (например используя median blending). Еще нужно бороться с освещением и заполнять пустые места если они остались (то что называется inpainting)

14-й слайд   Насчет текстуры, я думаю будет достаточно поговорить об одной но очень прорывной работе DensePose. 

Авторы данной работы, не только придумали  state-of-the-art метод предсказания текстуры, но и придумали очень умную систему разметки для того, чтобы собрать датку.

В целом идея такая - берется такая же архитектура как у MaskRCNN для паноптической сегментации, и добавляется дополнительная голова. 

У этой головы две задачи - сегментации 24 частей тела и регрессия UV-координат каждого пикселя заданной части тела на текстуре этой части тела. 

Потом эти текстуры частей тела можно простой трансформацией обьединить в одну текстуру.  Новый датасет DensePoseCOCO который собрали авторы статьи - это картинки с людьми с датасета COCO, и размеченными около 64-ью точками на кадре. Это выглядит приблизительно как на этих двух картинках.

Для остальных пикселей была использована teacher network - которая тренировалась на размеченных точках и потом предсказывала координаты для всех остальных точек.  Это бесспорно передовая работа - кроме предсказания текстуры, ее результаты используются на вход еще и в многих других системах , например в Tex2Shape что я обсуждал раньше.

Рекомендую к прочтению эту статью любому кого заинтересовала сфера 3Д после моего доклада. Ну еще для ознакомления, можно в поиске в слаке ОДС найти мой обзор в #article_essence 
Одна из немногочисленных проблем этого подхода - это то что авторы использовали свою IUV - систему координат, которая отличается от дефолтной SMPL UV-карты.

Поэтому в гитхабе можно найти множество неразрешенных issues - как сделать текстуры из DensePose для использования в Unity или Blender.  Нас в punch to face отсутствие готового решения конечно не остановило, и мы решили проблему использую опубликованные куски кода по трансфере текстур и достав оригинальную UV - карту с помощью питонового скрипта в блендере.

Для ускорения, я запустил вычисления функции перевода координат из IUV в UV для всех возможных IUV координат и сохранил результаты в dictionary, чтобы в следующий раз больше не вычислять эту долгую функцию. 

Еще чуть-чуть пошаманил с median blending на разных кадрах и вуаля получилась такая текстурка.


15-й слайд

У нас теперь есть моделька и текстурка, осталось только поместить бойца в сцену. 

Для этого нам нужно опять решить проблему сегментации. Это нам нужно для того чтобы выделить самую статичную часть сцены, которая станет якорем - центром системы координат нашего виртуального мира. 

Самый интуитивный выбор в ММА - это пол, вокруг него и крутится вся сцена, а сам он никуда не девается. 

Вообще фундаментальная проблема локализации в том, что без дополнительной информации невозможно отличить движения камеры и движения бойцов. Для проекции эти два движения идентичны.

Для этого мы можем трекать ключевые точки на полу, и по ним можно точно вычислить движение камеры.

Зная движение камеры и движение бойца на проекции, можно наконец-то вычислить и движение самого бойца в сцене.

16-й слайд

В punch-to-face мы решаем эту проблему вот так. 

Мы берем центр ринга за точку отсчета (0,0,0), и на первом кадре вручную размечаем 8 точек и сами задаем точные координаты этих точек в сцене, используя геометрическую информацию что это натуральный восьмиугольник.

Потом эти точки автоматически отслеживаются на последующих кадрах,  и на каждый кадр мы можем сделать “вид сверху” как на этой картинке, вычислив матрицу гомографии

Потом с помощью простых евристик и детектора 2Д позы, мы находим приблизительно вот эту точку между ног бойца, и эта координата (около 0.5 и 0.2) - и будет икс и игрик координатами нашего бойца в сцене. 

Координата z у нас к сожалению не 0, но зависит от роста бойца и вычисляется примерно по позиции копчика, потому что так устроена SMPL модель

17-й слайд

Так вот, решив проблемы позиционирования и семантической сегментации, и продемонстрировав эти результаты в нашем канале в ОДС,  добрые люди начали советовать, а почему бы уже сейчас не начать зарабатывать деньги на рекламе?

По-моему отличный совет. Делается это с помощью этих 6 простых шагов. 

Я в принципе уже объяснял как делаются Шаги 1,2,3 и 6, единственная деталь которую я недосказал - это то что когда мы делаем трансформацию в текстурный вид, мы можем еще делать и обратную трансформацию с текстуры в обычный вид. То есть мы можем взять любую текстуру и наложить ее обратно в кадр.

То есть все что нужно сделать, это нарисовать собственную текстуру пола, посчитать обратную матрицы гомографии, и обратно поместить текстуру в кадр. 

Таким образом мы добавили лого ОДС в ринг, а потом и вообще полностью заменили текстуру.

Только матрица гомографии должна быть вычислена очень точно, иначе текстура может “сьехать” как на этой картинке.

Поэтому мы в punch-to-face очень много времени и ресурсов инвестирую в проблему вычисления параметров камеры, и придумали достаточно интересные хаки.

Например, можно считать матрицу гомографии для каждой пары соседних кадров, и использовать ее для улучшения точности


18-й слайд

Теперь когда мы во всем с вами разобрались, хочу продемонстрировать чего можно добиться просто комбинируя готовые репозитории не дотренировывая сетки и с минимальными евристиками и костылями.
 
Здесь как видите мы использовали свои пол в сцене, и восстановили движение одного из бойцов во время первых 20 секунд боя. 

После 20 секунды вид в этом видео с ЮТуба меняется на другую камеру, и нужно использовать методы Re-Identification, чтобы “склеивать треки”, о которых я не рассказывал. 

В принципе это можно сделать  и руками…, но мы за полностью автоматизированные алгоритмы

Из готовых репозиториев иы использовали AlphaPose для детекции позы, HMMR и DensePose о которых я рассказывал ранее для воссозданию модели и текстуры соответсвенно, OpenCV для всего что связано с вычислением гомографии. 

Для тех кто в теме, для сегментации есть очень интересные и крутые ОДСовские фреймворки каталист и кекас, для тех кто не в теме могут за них прочитать в каналах tool_catalyst и tool_kekas соответсвенно.

Все результаты рендерелись в Блендере,  очень крутой OpenSource программе для 3Д моделирования и анимации

Результаты конечно очень далеки до идеала, но я надеюсь что я за это время что я тут стою, дал вам достаточно информации, чтобы вы могли потратить еще немного времени на дополнительный ресерч и сделать что-то похожее.

Возможно не в ММА, а в каком-то другом контексте с одним человеком в кадре - например реконструировать себя танцующего на видео с Ютуба с той самой вечерины.

Для тех кто хочет еще глубже погрузится в эту сферу, мы в punch-to-face продолжаем следить за новыми статьями и проводим свои собственные исследования. Мы стараемся как можно чаще радовать  читателей новыми информативными постами в нашем канале и обзорами в article_essence. 

Ну и конечно, двери в нашу команду всегда открыты для энтузиастов с горящими глазами!

19-й слайд (Бонус - не успею дойти, просто на всякий случай)

(Так, Я смотрю у нас осталось еще несколько минут и я могу перейти к бонусам и рассказать о том куда нужно дальше копать для улучшения качества нашей реконструкции)


Очень важная вещь для качественной реконструкции меша и текстуры - паноптическая сегментация бойцов и рефери. В целом сейчас есть два рабочих подхода чтобы решить эту проблему

Первый это MaskRCNN - о котором я говорил во время 2Д детекции. Все что меняется это после RoI Allign, тренируется сегментационная голова, а не 2Д keypoints голова. 

И она также страдает как top-down методы детекции 2Д позы - при перекрытии может детектится один трехногий человек как на картинке. 

Фундаментальная проблема - это то что людей нельзя разделить bboxam

Второй подход не имеет этой проблемы. 

Этот подход занят первое место в kaggle соревнование (я думаю здесь все знают что такое kaggle) DS bowl 2018, там была биологическая задача где нужно было отсегментировать ядра клеток, 

Трюк заключался в том чтобы натренировать UNet чтобы предсказывать границы объектов и потом с помощью пост-процессинга разбивать на инстансы

В нашей проблеме такой подход можно использовать, только потому что есть два человека в кадре! 

Можно просто использовать тот факт, что соседние кластеры относятся к разным бойцам и написать специальный алгоритм постпроцессинга. 

Проблема этого подхода - на тех данных что у меня пока есть, у меня не поучилось натренировать нормальный разделитель бойцов

20-й слайд

Недавно в 2019 появился 3-й подход к решению задачи паноптической сегментации людей - это Pose2Seg. 

Основная идея - это использовать 2Д деткцию позы, а не bbox для разделения двух людей. 

По gorund-truth данным подготавливаются кластеры позы, и меняется Roi - Allign на Affine-Align, то есть мы вырезаем не прямоугольный кусок, а из картинки а немножко по другому

Второе улучшение это использовать задекорированный скелет для фичей

Третье улучшение это новый спеициализированый датасет под данную проблему

В результате довольно неплохие результаты, но все равно далеко от идеала. 

Вообще авторы по хитрому поступили, они выдали результаты где на вход заходила настоящая 2Д поза, а не полученная из bottom-up детектора, что сильно завышает результаты. 

Из хорошего, если решится проблема 2Д позы, то с помощью этого метода одновременно решается и проблема сегментации

21-й слайд

Основной алгоритм для перехода с 2Д позы с нескольких ракурсов в 3Д позу - это алгебраическая триангуляция, и она требует несколько камер и их intrinsic и extrinsic параметры. 

С одной камерой возможно несколько глобальных минимумом - например повернуть человек на 180 градусов. 

Оптимизацией ищется 3Д поза которая минимизирует reprоjection error с разных ракурсов. Intrinsic и extrinsic параметры это две матрицы которые нужны для перехода из 3Д координат кадра в координаты картинки. 

Но при большом количестве перекрытия, детекторы 2Д позы ведут себя сильно шумно, судя из предидущего слайда. 

Поэтому в работе MVPose, используется алгоритм cycle-consistent кластеризации чтобы найти кластеры поз с разных камер. 

Судя по картинке, на CMU Panoptic (датасет без ground truth) система достигает хороших результатов


В 2019 году появились системы которые могут угадывать 3Д позу по одной картинке с помощью нейронных сеток, прям как это делают люди. 

По-моему это большой прорыв, который открывает множество новых применений технологии реконструкции. 

EpipolarPose при тренировке не требует 3Д ground truth, только несколько 2Д с разных ракурсов, и одновременно оптимизирует параметры камеры и позу. 

На инференсе можно уже подавать одну картинку, и алгоритм в простых случаях хорошо справляется

VideoPose3D - существующая SOTA от Facebook использует информацию по видео, для сглаживания скелета, но очень нестабильна если в кадре появляется другой человек. 

Это в принципе проблема почти всех существующих систем на данный момент.  Большая проблема в 3Д резерве это нехватка датасетов, самые распространенный это Motion Capture датасет Human3.6M, в нем 3 миллиона картинок людей с датчиками в разных позах 

Campus, Shelf - датасеты с несколькими откалиброванным камерами. В первом 3 человека общается около входа в институт, во втором четыре собирают полку. 

Такого экшена как в MMA нигде нет
